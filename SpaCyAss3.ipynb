{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-26T12:33:12.236682Z",
     "start_time": "2024-09-26T12:33:08.333778Z"
    }
   },
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Print details about the pipeline components\n",
    "print(\"Pipeline components:\", nlp.pipe_names)\n",
    "\n",
    "print(\"\\nDetailed pipeline info:\")\n",
    "for name, component in nlp.pipeline:\n",
    "    print(f\"Component name: {name}, Component: {component}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "\n",
      "Detailed pipeline info:\n",
      "Component name: tok2vec, Component: <spacy.pipeline.tok2vec.Tok2Vec object at 0x0000025DBDA72BE0>\n",
      "Component name: tagger, Component: <spacy.pipeline.tagger.Tagger object at 0x0000025DBDA72400>\n",
      "Component name: parser, Component: <spacy.pipeline.dep_parser.DependencyParser object at 0x0000025DBD9946D0>\n",
      "Component name: attribute_ruler, Component: <spacy.pipeline.attributeruler.AttributeRuler object at 0x0000025DBDC4AD40>\n",
      "Component name: lemmatizer, Component: <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x0000025DBDCA0440>\n",
      "Component name: ner, Component: <spacy.pipeline.ner.EntityRecognizer object at 0x0000025DBD994510>\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T12:38:39.619262Z",
     "start_time": "2024-09-26T12:38:39.076331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@Language.component(\"longest_token\")\n",
    "def longest_token_function(doc):\n",
    "    longest_token_len = 0\n",
    "    longest_token = \"\"\n",
    "\n",
    "    for token in doc:\n",
    "        if len(token.text) > longest_token_len:\n",
    "            longest_token_len = len(token.text)\n",
    "            longest_token = token.text\n",
    "\n",
    "    print(f\"Longest token: {longest_token} (Length: {longest_token_len})\")\n",
    "\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"longest_token\", first=True)\n",
    "\n",
    "text = \"Artificial Intelligence is transforming the future of technology.\"\n",
    "doc = nlp(text)"
   ],
   "id": "824260fca53d3366",
   "outputs": [],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
